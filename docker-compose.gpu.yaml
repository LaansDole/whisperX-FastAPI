version: '3.8'

services:
  # Temporal Server (same as CPU version)
  temporal:
    image: temporalio/auto-setup:latest
    ports:
      - "7233:7233"
      - "8233:8233"  # Web UI
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=postgres
    depends_on:
      - postgres
    networks:
      - whisperx-network
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "cluster", "health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  postgres:
    image: postgres:13
    environment:
      - POSTGRES_PASSWORD=temporal
      - POSTGRES_USER=temporal
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - whisperx-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U temporal"]
      interval: 10s
      timeout: 5s
      retries: 5

  # WhisperX FastAPI Application with GPU
  whisperx-api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - INSTALL_GPU=true  # Enable GPU support
    ports:
      - "8000:8000"
    environment:
      # GPU-optimized settings
      - HF_TOKEN=${HF_TOKEN:-}
      - WHISPER_MODEL=${WHISPER_MODEL:-small}
      - DEFAULT_LANG=${DEFAULT_LANG:-en}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - FILTER_WARNING=${FILTER_WARNING:-true}
      - DEVICE=${DEVICE:-cuda}  # Use CUDA by default
      - COMPUTE_TYPE=${COMPUTE_TYPE:-float16}  # GPU-optimized precision
      - DIARIZATION_MODEL_PATH=${DIARIZATION_MODEL_PATH:-}
      # Temporal Configuration
      - TEMPORAL_SERVER_URL=temporal:7233
      - TEMPORAL_NAMESPACE=${TEMPORAL_NAMESPACE:-default}
      - TEMPORAL_TASK_QUEUE=${TEMPORAL_TASK_QUEUE:-whisperx-task-queue}
      # Retry Policy Configuration
      - TEMPORAL_MAX_ATTEMPTS=${TEMPORAL_MAX_ATTEMPTS:-3}
      - TEMPORAL_INITIAL_INTERVAL=${TEMPORAL_INITIAL_INTERVAL:-5}
      - TEMPORAL_BACKOFF_COEFFICIENT=${TEMPORAL_BACKOFF_COEFFICIENT:-2.0}
      - TEMPORAL_MAX_INTERVAL=${TEMPORAL_MAX_INTERVAL:-300}
      # Activity Timeout Configuration (in minutes) - reduced for GPU
      - TRANSCRIPTION_TIMEOUT=${TRANSCRIPTION_TIMEOUT:-15}
      - ALIGNMENT_TIMEOUT=${ALIGNMENT_TIMEOUT:-5}
      - DIARIZATION_TIMEOUT=${DIARIZATION_TIMEOUT:-5}
      - SPEAKER_ASSIGNMENT_TIMEOUT=${SPEAKER_ASSIGNMENT_TIMEOUT:-3}
    volumes:
      # Persist model cache to avoid re-downloading
      - whisperx-huggingface-cache:/root/.cache/huggingface
      - whisperx-torch-cache:/root/.cache/torch
      # Mount for file uploads (optional)
      - ./uploads:/tmp/uploads
    depends_on:
      temporal:
        condition: service_healthy
    networks:
      - whisperx-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Temporal Worker with GPU
  whisperx-worker:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - INSTALL_GPU=true  # Enable GPU support
    environment:
      # GPU-optimized settings (same as API)
      - HF_TOKEN=${HF_TOKEN:-}
      - WHISPER_MODEL=${WHISPER_MODEL:-small}
      - DEFAULT_LANG=${DEFAULT_LANG:-en}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - FILTER_WARNING=${FILTER_WARNING:-true}
      - DEVICE=${DEVICE:-cuda}  # Use CUDA by default
      - COMPUTE_TYPE=${COMPUTE_TYPE:-float16}  # GPU-optimized precision
      - DIARIZATION_MODEL_PATH=${DIARIZATION_MODEL_PATH:-}
      # Temporal Configuration
      - TEMPORAL_SERVER_URL=temporal:7233
      - TEMPORAL_NAMESPACE=${TEMPORAL_NAMESPACE:-default}
      - TEMPORAL_TASK_QUEUE=${TEMPORAL_TASK_QUEUE:-whisperx-task-queue}
      # Retry Policy Configuration
      - TEMPORAL_MAX_ATTEMPTS=${TEMPORAL_MAX_ATTEMPTS:-3}
      - TEMPORAL_INITIAL_INTERVAL=${TEMPORAL_INITIAL_INTERVAL:-5}
      - TEMPORAL_BACKOFF_COEFFICIENT=${TEMPORAL_BACKOFF_COEFFICIENT:-2.0}
      - TEMPORAL_MAX_INTERVAL=${TEMPORAL_MAX_INTERVAL:-300}
      # Activity Timeout Configuration (in minutes) - reduced for GPU
      - TRANSCRIPTION_TIMEOUT=${TRANSCRIPTION_TIMEOUT:-15}
      - ALIGNMENT_TIMEOUT=${ALIGNMENT_TIMEOUT:-5}
      - DIARIZATION_TIMEOUT=${DIARIZATION_TIMEOUT:-5}
      - SPEAKER_ASSIGNMENT_TIMEOUT=${SPEAKER_ASSIGNMENT_TIMEOUT:-3}
    volumes:
      # Share model cache with API service
      - whisperx-huggingface-cache:/root/.cache/huggingface
      - whisperx-torch-cache:/root/.cache/torch
      # Mount for file processing (optional)
      - ./uploads:/tmp/uploads
    command: ["uv", "run", "python", "-m", "app.temporal_worker"]
    depends_on:
      temporal:
        condition: service_healthy
    networks:
      - whisperx-network
    restart: unless-stopped
    deploy:
      replicas: 1  # Increase for more worker instances
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  whisperx-network:
    driver: bridge

volumes:
  temporal-data:
  postgres-data:
  whisperx-huggingface-cache:
  whisperx-torch-cache: