{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaansDole/whisperX-FastAPI/blob/main/notebooks/whisperx_fastapi_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgRmhCYcFPv9"
      },
      "source": [
        "# WhisperX FastAPI on Google Colab\n",
        "\n",
        "This notebook sets up and runs the WhisperX FastAPI project on Google Colab, utilizing its GPU for speech-to-text processing. The API service is exposed through a Cloudflare tunnel to allow external access.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Speech-to-text transcription\n",
        "- Audio alignment\n",
        "- Speaker diarization\n",
        "- Combined services\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Google Colab with GPU runtime\n",
        "- Hugging Face token for model access\n",
        "- Cloudflare account (free tier works fine)\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Make sure you're running this notebook with GPU runtime\n",
        "2. Execute each cell in order\n",
        "3. Use the Cloudflare tunnel URL to access the API\n",
        "\n",
        "Let's start by checking if we have GPU access and setting up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "UowbTabZFS0k",
        "outputId": "5d97f692-0396-4060-86e1-574c8362cba3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0KVaZ_zFPwC"
      },
      "source": [
        "## 1. Install System Dependencies\n",
        "\n",
        "First, we need to install the required system packages and utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A9foV_tYFPwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14cc07a4-2c43-4b64-82d6-d0826214af4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.82)] [Connected to cloud.r-p\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libcudnn8 is already the newest version (8.9.7.29-1+cuda12.2).\n",
            "libcudnn8-dev is already the newest version (8.9.7.29-1+cuda12.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install ffmpeg for audio/video processing\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "\n",
        "# Install git and other utilities\n",
        "!apt-get install -y git curl wget\n",
        "\n",
        "!apt update\n",
        "!apt install libcudnn8 libcudnn8-dev -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfCgeb4JFPwC"
      },
      "source": [
        "## 2. Clone the WhisperX FastAPI Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlMBW9VAFPwC",
        "outputId": "5c10c333-0e05-489f-944f-d2f987d36cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whisperX-FastAPI'...\n",
            "remote: Enumerating objects: 1336, done.\u001b[K\n",
            "remote: Counting objects: 100% (524/524), done.\u001b[K\n",
            "remote: Compressing objects: 100% (244/244), done.\u001b[K\n",
            "remote: Total 1336 (delta 361), reused 317 (delta 278), pack-reused 812 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1336/1336), 40.54 MiB | 44.54 MiB/s, done.\n",
            "Resolving deltas: 100% (730/730), done.\n",
            "total 116\n",
            "drwxr-xr-x 11 root root  4096 Jun 25 14:12 .\n",
            "drwxr-xr-x  1 root root  4096 Jun 25 14:12 ..\n",
            "drwxr-xr-x  4 root root  4096 Jun 25 14:12 app\n",
            "drwxr-xr-x  2 root root  4096 Jun 25 14:12 .devcontainer\n",
            "-rw-r--r--  1 root root   531 Jun 25 14:12 docker-compose.yml\n",
            "-rw-r--r--  1 root root  1627 Jun 25 14:12 dockerfile\n",
            "-rw-r--r--  1 root root   331 Jun 25 14:12 .dockerignore\n",
            "-rw-r--r--  1 root root   727 Jun 25 14:12 .env.example\n",
            "drwxr-xr-x  8 root root  4096 Jun 25 14:12 .git\n",
            "drwxr-xr-x  3 root root  4096 Jun 25 14:12 .github\n",
            "-rw-r--r--  1 root root   168 Jun 25 14:12 .gitignore\n",
            "-rw-r--r--  1 root root   207 Jun 25 14:12 .gitleaks.toml\n",
            "-rw-r--r--  1 root root  1070 Jun 25 14:12 LICENSE\n",
            "-rw-r--r--  1 root root    39 Jun 25 14:12 .markdownlint.json\n",
            "drwxr-xr-x  2 root root  4096 Jun 25 14:12 notebooks\n",
            "-rw-r--r--  1 root root  1963 Jun 25 14:12 .pre-commit-config.yaml\n",
            "-rw-r--r--  1 root root   532 Jun 25 14:12 pyproject.toml\n",
            "-rw-r--r--  1 root root 12792 Jun 25 14:12 README.md\n",
            "drwxr-xr-x  2 root root  4096 Jun 25 14:12 requirements\n",
            "-rw-r--r--  1 root root   153 Jun 25 14:12 requirements.txt\n",
            "drwxr-xr-x  2 root root  4096 Jun 25 14:12 scripts\n",
            "-rw-r--r--  1 root root    86 Jun 25 14:12 setup.cfg\n",
            "-rw-r--r--  1 root root  5371 Jun 25 14:12 start_server.py\n",
            "drwxr-xr-x  3 root root  4096 Jun 25 14:12 tests\n",
            "drwxr-xr-x  2 root root  4096 Jun 25 14:12 .vscode\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!rm -rf whisperX-FastAPI\n",
        "!git clone https://github.com/LaansDole/whisperX-FastAPI.git\n",
        "!cd whisperX-FastAPI && ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueqDc01EFPwD"
      },
      "source": [
        "## 3. Install Dependencies\n",
        "\n",
        "We'll install PyTorch with CUDA support and all required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test script to verify PyTorch installation and CUDA availability\n",
        "\"\"\"\n",
        "import sys\n",
        "\n",
        "def test_torch_installation():\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"âœ“ PyTorch installed successfully: {torch.__version__}\")\n",
        "\n",
        "        # Test CUDA availability\n",
        "        if hasattr(torch, 'cuda'):\n",
        "            if torch.cuda.is_available():\n",
        "                print(f\"âœ“ CUDA is available: {torch.cuda.get_device_name(0)}\")\n",
        "                print(f\"âœ“ CUDA version: {torch.version.cuda}\")\n",
        "            else:\n",
        "                print(\"âš  CUDA is not available, will use CPU\")\n",
        "        else:\n",
        "            print(\"âœ— torch.cuda module not found - PyTorch installation is corrupted\")\n",
        "            return False\n",
        "\n",
        "        # Test basic tensor operations\n",
        "        x = torch.randn(3, 3)\n",
        "        print(f\"âœ“ Basic tensor operations work: {x.shape}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"âœ— Failed to import PyTorch: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— PyTorch test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def test_numpy_installation():\n",
        "    try:\n",
        "        import numpy as np\n",
        "        print(f\"âœ“ NumPy installed successfully: {np.__version__}\")\n",
        "\n",
        "        # Test basic operations\n",
        "        arr = np.array([1, 2, 3])\n",
        "        print(f\"âœ“ Basic NumPy operations work: {arr.shape}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"âœ— Failed to import NumPy: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— NumPy test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing PyTorch and NumPy installation...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    numpy_ok = test_numpy_installation()\n",
        "    torch_ok = test_torch_installation()\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    if numpy_ok and torch_ok:\n",
        "        print(\"âœ“ All tests passed! Environment is ready.\")\n",
        "    else:\n",
        "        print(\"âœ— Some tests failed. Please check the installation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UHM5_KmUEEV",
        "outputId": "880b7a38-b18e-4964-fb84-6552b0547e31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing PyTorch and NumPy installation...\n",
            "==================================================\n",
            "âœ“ NumPy installed successfully: 2.0.2\n",
            "âœ“ Basic NumPy operations work: (3,)\n",
            "âœ“ PyTorch installed successfully: 2.6.0+cu124\n",
            "âœ“ CUDA is available: Tesla T4\n",
            "âœ“ CUDA version: 12.4\n",
            "âœ“ Basic tensor operations work: torch.Size([3, 3])\n",
            "==================================================\n",
            "âœ“ All tests passed! Environment is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJrF6J1tFPwD"
      },
      "outputs": [],
      "source": [
        "# Install project requirements\n",
        "!cd whisperX-FastAPI && pip install -r requirements/prod.txt\n",
        "\n",
        "# Install additional packages for Colab environment\n",
        "!cd whisperX-FastAPI && pip install colorlog pyngrok python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJQd3yTqFPwD"
      },
      "source": [
        "## 4. Set Up Environment Variables\n",
        "\n",
        "Configure the required environment variables for WhisperX. You'll need to enter your Hugging Face API token to access the models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Hugging Face API token\n",
        "1. Go to your Hugging Face token settings page.\n",
        "2. Select the token you are using.\n",
        "3. Under the \"Token permissions\" section, make sure that \"Read access to public gated repositories\" is enabled **[IMPORTANT]**.\n",
        "4. Save the changes to your token.\n",
        "\n"
      ],
      "metadata": {
        "id": "RCwqHIbMrmC8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24342fd9"
      },
      "source": [
        "To add your Hugging Face token as a secret in Google Colab:\n",
        "\n",
        "1.  Go to your Hugging Face settings page: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "2.  Create a new token or copy an existing one.\n",
        "3.  In your Google Colab notebook, click on the \"ðŸ”‘ Secrets\" tab in the left sidebar.\n",
        "4.  Click on \"Add new secret\".\n",
        "5.  For the \"Name\" field, enter `HF_TOKEN`.\n",
        "6.  For the \"Value\" field, paste your Hugging Face token.\n",
        "7.  Make sure the \"Notebook access\" toggle is enabled for this notebook.\n",
        "8.  Restart your Colab session by going to \"Runtime\" -> \"Restart session\".\n",
        "\n",
        "Once you have followed these steps, the `HF_TOKEN` secret will be available in your notebook and the warning message should disappear after restarting the runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MokzptrhTPKJ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "def download_model(model_name, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Downloads a model from the Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model to download.\n",
        "        cache_dir (str, optional): The directory to cache the model in. Defaults to None.\n",
        "    \"\"\"\n",
        "    print(f\"Downloading model: {model_name}\")\n",
        "    try:\n",
        "        snapshot_download(\n",
        "            repo_id=model_name,\n",
        "            cache_dir=cache_dir,\n",
        "            token=os.environ.get(\"HF_TOKEN\"),\n",
        "        )\n",
        "        print(f\"Model '{model_name}' downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model '{model_name}': {e}\")\n",
        "\n",
        "# Directly call the download function with the desired model name\n",
        "download_model(model_name=\"pyannote/speaker-diarization-3.1\", cache_dir=\"models/pyannote/speaker-diarization-3.1\")"
      ],
      "metadata": {
        "id": "wos4bjuapKFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyTAXbREFPwD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if we're already in the whisperX-FastAPI directory\n",
        "current_dir = os.path.basename(os.getcwd())\n",
        "if current_dir != \"whisperX-FastAPI\":\n",
        "    os.chdir(\"whisperX-FastAPI\")\n",
        "    print(f\"Changed directory to whisperX-FastAPI\")\n",
        "else:\n",
        "    print(\"Already in whisperX-FastAPI directory\")\n",
        "\n",
        "# Enter your Hugging Face token here\n",
        "HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# Choose Whisper model size\n",
        "WHISPER_MODEL = input(\"Enter Whisper model size (default: tiny): \") or \"tiny\"\n",
        "\n",
        "# Set log level\n",
        "LOG_LEVEL = \"INFO\"\n",
        "\n",
        "# Create .env file\n",
        "env_content = f\"\"\"HF_TOKEN={HF_TOKEN}\n",
        "WHISPER_MODEL={WHISPER_MODEL}\n",
        "LOG_LEVEL={LOG_LEVEL}\n",
        "DEVICE=cuda\n",
        "COMPUTE_TYPE=float16\n",
        "DB_URL=sqlite:///records.db\n",
        "\"\"\"\n",
        "\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "print(\"Environment configuration completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env"
      ],
      "metadata": {
        "id": "SFbTdZARZ-kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji-K4diXFPwE"
      },
      "source": [
        "## 5. Start the FastAPI Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-VVKLbgTPKK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from google.colab.output import serve_kernel_port_as_iframe\n",
        "\n",
        "# --- Configuration ---\n",
        "PORT = 8000\n",
        "LOG_CONFIG_PATH = \"app/uvicorn_log_conf.yaml\"\n",
        "APP_MODULE = \"app.main:app\"\n",
        "\n",
        "# --- Global variable to hold the server process ---\n",
        "server_process = None\n",
        "\n",
        "def kill_port(port):\n",
        "    \"\"\"Kills any process listening on the given port.\"\"\"\n",
        "    print(f\"Checking for and terminating any process on port {port}...\")\n",
        "    try:\n",
        "        result = subprocess.run([\"lsof\", \"-ti\", f\":{port}\"], capture_output=True, text=True)\n",
        "        if result.stdout:\n",
        "            pids = result.stdout.strip().split('\\n')\n",
        "            for pid in pids:\n",
        "                try:\n",
        "                    os.kill(int(pid), signal.SIGKILL)\n",
        "                    print(f\"Killed process {pid} on port {port}.\")\n",
        "                except (ProcessLookupError, ValueError):\n",
        "                    pass  # Process already gone\n",
        "    except FileNotFoundError:\n",
        "        print(\"`lsof` command not found. Skipping port clearing.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while trying to kill port {port}: {e}\")\n",
        "\n",
        "def start_server():\n",
        "    \"\"\"Starts the Uvicorn server in a background thread.\"\"\"\n",
        "    global server_process\n",
        "\n",
        "    # Ensure we are in the correct directory\n",
        "    if os.path.basename(os.getcwd()) != \"whisperX-FastAPI\":\n",
        "        os.chdir(\"whisperX-FastAPI\")\n",
        "        print(\"Changed directory to whisperX-FastAPI\")\n",
        "\n",
        "    # First, ensure the port is free\n",
        "    kill_port(PORT)\n",
        "\n",
        "    # Command to start Uvicorn\n",
        "    command = [\n",
        "        \"uvicorn\",\n",
        "        APP_MODULE,\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", str(PORT),\n",
        "        \"--log-config\", LOG_CONFIG_PATH,\n",
        "        \"--log-level\", \"info\"\n",
        "    ]\n",
        "\n",
        "    # Start the server as a background process\n",
        "    print(\"Starting FastAPI server...\")\n",
        "    server_process = subprocess.Popen(command)\n",
        "    print(f\"Server process started with PID: {server_process.pid}\")\n",
        "\n",
        "    # Wait a moment for the server to initialize\n",
        "    time.sleep(12)\n",
        "\n",
        "    # Expose the port to a public URL\n",
        "    print(f\"Exposing port {PORT} as an iframe...\")\n",
        "    serve_kernel_port_as_iframe(port=PORT, height=800)\n",
        "\n",
        "def stop_server():\n",
        "    \"\"\"Stops the background Uvicorn server.\"\"\"\n",
        "    global server_process\n",
        "    if server_process:\n",
        "        print(f\"Stopping server process with PID: {server_process.pid}...\")\n",
        "        server_process.terminate()\n",
        "        try:\n",
        "            # Wait for the process to terminate\n",
        "            server_process.wait(timeout=10)\n",
        "            print(\"Server stopped successfully.\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"Server did not terminate gracefully. Forcing shutdown...\")\n",
        "            server_process.kill()\n",
        "            print(\"Server forced to shut down.\")\n",
        "        server_process = None\n",
        "    else:\n",
        "        print(\"Server is not running.\")\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        start_server()\n",
        "        # The server is running in the background.\n",
        "        # The script will keep running, allowing the server to stay active.\n",
        "        # To stop the server, you would call stop_server() in another cell.\n",
        "        print(\"\\nServer is running in the background.\")\n",
        "        print(\"To stop the server, call the stop_server() function.\")\n",
        "        # Keep the main thread alive\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nKeyboard interrupt received. Shutting down server...\")\n",
        "        stop_server()\n",
        "        print(\"Shutdown complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d-lXHtOKpJ17"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}