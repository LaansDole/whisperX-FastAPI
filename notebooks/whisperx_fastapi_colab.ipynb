{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperX FastAPI on Google Colab\n",
    "\n",
    "This notebook sets up and runs the WhisperX FastAPI project on Google Colab, utilizing its GPU for speech-to-text processing. The API service is exposed through a Cloudflare tunnel to allow external access.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Speech-to-text transcription\n",
    "- Audio alignment\n",
    "- Speaker diarization\n",
    "- Combined services\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab with GPU runtime\n",
    "- Hugging Face token for model access\n",
    "- Cloudflare account (free tier works fine)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Make sure you're running this notebook with GPU runtime\n",
    "2. Execute each cell in order\n",
    "3. Use the Cloudflare tunnel URL to access the API\n",
    "\n",
    "Let's start by checking if we have GPU access and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install System Dependencies\n",
    "\n",
    "First, we need to install the required system packages and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ffmpeg for audio/video processing\n",
    "!apt-get update && apt-get install -y ffmpeg\n",
    "\n",
    "# Install git and other utilities\n",
    "!apt-get install -y git curl wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone the WhisperX FastAPI Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/pavelzbornik/whisperX-FastAPI.git\n",
    "!cd whisperX-FastAPI && ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Python Virtual Environment and Install Dependencies\n",
    "\n",
    "We'll install PyTorch with CUDA support and all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and activate a virtual environment\n",
    "!cd whisperX-FastAPI && python -m venv venv\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install --upgrade pip\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install project requirements\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install -r requirements/dev.txt\n",
    "\n",
    "# Install additional packages for Colab environment\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install colorlog pyngrok python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Environment Variables\n",
    "\n",
    "Configure the required environment variables for WhisperX. You'll need to enter your Hugging Face API token to access the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enter your Hugging Face token here\n",
    "HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Choose Whisper model size\n",
    "WHISPER_MODEL = input(\"Enter Whisper model size (default: tiny): \") or \"tiny\"\n",
    "\n",
    "# Set log level\n",
    "LOG_LEVEL = \"INFO\"\n",
    "\n",
    "# Create .env file\n",
    "env_content = f\"\"\"HF_TOKEN={HF_TOKEN}\n",
    "WHISPER_MODEL={WHISPER_MODEL}\n",
    "LOG_LEVEL={LOG_LEVEL}\n",
    "DEVICE=cuda\n",
    "COMPUTE_TYPE=float16\n",
    "DB_URL=sqlite:///records.db\n",
    "\"\"\"\n",
    "\n",
    "with open(\"whisperX-FastAPI/.env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"Environment configuration completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install and Configure Cloudflare Tunnel\n",
    "\n",
    "We'll use Cloudflare tunnels to expose the FastAPI service to the internet, allowing you to access it from your browser or other clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install cloudflared\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb\n",
    "\n",
    "print(\"Cloudflare tunnel client installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start the FastAPI Service\n",
    "\n",
    "Now we'll run the FastAPI application in the background and expose it through the Cloudflare tunnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run FastAPI in the background\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import IPython.display as display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Start FastAPI server in a separate process\n",
    "def start_fastapi_server():\n",
    "    print(\"Starting FastAPI server...\")\n",
    "    # Command to start the FastAPI application\n",
    "    os.chdir(\"whisperX-FastAPI\")\n",
    "    command = \"source venv/bin/activate && uvicorn app.main:app --host 0.0.0.0 --port 8000 --log-config app/uvicorn_log_conf.yaml\"\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(\"FastAPI server started. Waiting for it to initialize...\")\n",
    "    time.sleep(5)  # Give some time for the server to start\n",
    "    os.chdir(\"..\")\n",
    "    return process\n",
    "\n",
    "# Start Cloudflare tunnel in a separate process\n",
    "def start_cloudflare_tunnel():\n",
    "    print(\"Starting Cloudflare tunnel...\")\n",
    "    tunnel_process = subprocess.Popen(\n",
    "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8000\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True\n",
    "    )\n",
    "    \n",
    "    # Extract the tunnel URL from the output\n",
    "    tunnel_url = None\n",
    "    for line in tunnel_process.stdout:\n",
    "        print(line.strip())\n",
    "        if \"https://\" in line and \"trycloudflare.com\" in line:\n",
    "            tunnel_url = line.strip().split(\" \")[-1]\n",
    "            break\n",
    "    \n",
    "    return tunnel_process, tunnel_url\n",
    "\n",
    "# Define global processes to keep them alive\n",
    "fastapi_process = None\n",
    "tunnel_process = None\n",
    "tunnel_url = None\n",
    "\n",
    "# Start the services\n",
    "try:\n",
    "    fastapi_process = start_fastapi_server()\n",
    "    time.sleep(2)  # Wait for FastAPI to start\n",
    "    tunnel_process, tunnel_url = start_cloudflare_tunnel()\n",
    "    \n",
    "    # Display the tunnel URL with a clickable link\n",
    "    if tunnel_url:\n",
    "        print(f\"\\n‚ú® WhisperX FastAPI is now running and accessible at: {tunnel_url}\")\n",
    "        print(f\"\\nüìù API Documentation: {tunnel_url}/docs\")\n",
    "        display.display(display.HTML(f'<a href=\"{tunnel_url}/docs\" target=\"_blank\">Open API Documentation</a>'))\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Failed to get tunnel URL. Check the output above for errors.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error starting services: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor GPU Usage\n",
    "\n",
    "You can monitor GPU usage while the service is running to ensure it's properly utilizing the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU usage\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the API\n",
    "\n",
    "Here's an example of how to use the API to transcribe an audio file uploaded to Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.colab import files\n",
    "\n",
    "# Function to upload a file to the WhisperX API\n",
    "def transcribe_audio(file_path, api_url):\n",
    "    # API endpoint for speech-to-text\n",
    "    endpoint = f\"{api_url}/speech-to-text\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        \"model\": WHISPER_MODEL,  # Use the same model as configured earlier\n",
    "        \"language\": \"en\"  # Change this to match your audio language\n",
    "    }\n",
    "    \n",
    "    # Create the multipart form data\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        files = {\"audio_file\": (os.path.basename(file_path), audio_file)}\n",
    "        response = requests.post(endpoint, params=params, files=files)\n",
    "    \n",
    "    # Return the response\n",
    "    return response.json()\n",
    "\n",
    "# Upload an audio file\n",
    "print(\"Please upload an audio file (supported formats: mp3, wav, m4a, etc.)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Process the uploaded file\n",
    "if uploaded:\n",
    "    file_name = list(uploaded.keys())[0]\n",
    "    \n",
    "    # Check if tunnel URL is available\n",
    "    if tunnel_url:\n",
    "        print(f\"Transcribing {file_name}...\")\n",
    "        result = transcribe_audio(file_name, tunnel_url)\n",
    "        \n",
    "        # Display the identifier for checking the task status\n",
    "        if \"identifier\" in result:\n",
    "            print(f\"\\nTask identifier: {result['identifier']}\")\n",
    "            print(f\"\\nCheck the task status at: {tunnel_url}/task/{result['identifier']}\")\n",
    "            display.display(display.HTML(f'<a href=\"{tunnel_url}/task/{result[\"identifier\"]}\" target=\"_blank\">Check Task Status</a>'))\n",
    "        else:\n",
    "            print(\"Error:\", result)\n",
    "    else:\n",
    "        print(\"Tunnel URL is not available. Make sure the FastAPI service and Cloudflare tunnel are running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check Task Status\n",
    "\n",
    "Use this cell to check the status of your transcription task using the task identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check task status\n",
    "def check_task_status(task_id, api_url):\n",
    "    endpoint = f\"{api_url}/task/{task_id}\"\n",
    "    response = requests.get(endpoint)\n",
    "    return response.json()\n",
    "\n",
    "# Enter task identifier\n",
    "task_id = input(\"Enter task identifier: \")\n",
    "\n",
    "# Check task status\n",
    "if tunnel_url and task_id:\n",
    "    status = check_task_status(task_id, tunnel_url)\n",
    "    print(\"Task Status:\")\n",
    "    import json\n",
    "    print(json.dumps(status, indent=2))\n",
    "else:\n",
    "    print(\"Tunnel URL or task ID is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Shutdown Services\n",
    "\n",
    "When you're done, use this cell to properly shut down the services and free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shut down services\n",
    "def shutdown_services():\n",
    "    global fastapi_process, tunnel_process\n",
    "    \n",
    "    print(\"Shutting down services...\")\n",
    "    \n",
    "    # Terminate Cloudflare tunnel\n",
    "    if tunnel_process:\n",
    "        tunnel_process.terminate()\n",
    "        tunnel_process.wait()\n",
    "        print(\"Cloudflare tunnel stopped.\")\n",
    "    \n",
    "    # Terminate FastAPI server\n",
    "    if fastapi_process:\n",
    "        fastapi_process.terminate()\n",
    "        fastapi_process.wait()\n",
    "        print(\"FastAPI server stopped.\")\n",
    "    \n",
    "    print(\"All services have been shut down.\")\n",
    "\n",
    "# Shut down the services\n",
    "shutdown_services()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup\n",
    "\n",
    "Finally, clean up any temporary files and free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "!rm -f cloudflared-linux-amd64.deb\n",
    "\n",
    "# Free up GPU memory (if any is still in use)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared.\")\n",
    "\n",
    "print(\"Cleanup completed. You can now close this notebook or run it again if needed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
