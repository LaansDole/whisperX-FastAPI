{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperX FastAPI on Google Colab\n",
    "\n",
    "This notebook sets up and runs the WhisperX FastAPI project on Google Colab, utilizing its GPU for speech-to-text processing. The API service is exposed through a Cloudflare tunnel to allow external access.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Speech-to-text transcription\n",
    "- Audio alignment\n",
    "- Speaker diarization\n",
    "- Combined services\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab with GPU runtime\n",
    "- Hugging Face token for model access\n",
    "- Cloudflare account (free tier works fine)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Make sure you're running this notebook with GPU runtime\n",
    "2. Execute each cell in order\n",
    "3. Use the Cloudflare tunnel URL to access the API\n",
    "\n",
    "Let's start by checking if we have GPU access and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available using multiple methods",
    "import torch",
    "import subprocess",
    "import platform",
    "",
    "print('=== GPU Availability Check ===')",
    "",
    "# Method 1: PyTorch CUDA check",
    "print(f'PyTorch version: {torch.__version__}')",
    "print(f'CUDA available in PyTorch: {torch.cuda.is_available()}')",
    "",
    "if torch.cuda.is_available():",
    "    print(f'CUDA version: {torch.version.cuda}')",
    "    print(f'Number of GPUs: {torch.cuda.device_count()}')",
    "    for i in range(torch.cuda.device_count()):",
    "        gpu_name = torch.cuda.get_device_name(i)",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3",
    "        print(f'GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)')",
    "        ",
    "    # Set device",
    "    device = torch.device('cuda')",
    "    print(f'Current device: {device}')",
    "    ",
    "    # Test GPU with a simple tensor operation",
    "    try:",
    "        test_tensor = torch.randn(1000, 1000).to(device)",
    "        result = torch.matmul(test_tensor, test_tensor)",
    "        print('\u2705 GPU tensor operations working!')",
    "        del test_tensor, result",
    "        torch.cuda.empty_cache()",
    "    except Exception as e:",
    "        print(f'\u274c GPU tensor operation failed: {e}')",
    "else:",
    "    print('\u274c No CUDA-capable GPU found')",
    "    device = torch.device('cpu')",
    "    print(f'Falling back to CPU: {device}')",
    "",
    "# Method 2: Check for nvidia-ml-py (if available)",
    "try:",
    "    import pynvml",
    "    pynvml.nvmlInit()",
    "    gpu_count = pynvml.nvmlDeviceGetCount()",
    "    print(f'",
    "NVML GPU count: {gpu_count}')",
    "    for i in range(gpu_count):",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)",
    "        name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')",
    "        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)",
    "        print(f'GPU {i}: {name}')",
    "        print(f'  Memory: {memory_info.used/1024**3:.1f}GB / {memory_info.total/1024**3:.1f}GB')",
    "except ImportError:",
    "    print('",
    "Note: pynvml not available for detailed GPU info')",
    "except Exception as e:",
    "    print(f'",
    "NVML error: {e}')",
    "",
    "# Method 3: System info",
    "print(f'",
    "System: {platform.system()} {platform.release()}')",
    "print(f'Python: {platform.python_version()}')",
    "",
    "# Method 4: Try nvidia-smi if available (fallback)",
    "try:",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used', '--format=csv,noheader,nounits'], ",
    "                          capture_output=True, text=True, timeout=10)",
    "    if result.returncode == 0:",
    "        print('",
    "=== nvidia-smi output ===')",
    "        lines = result.stdout.strip().split('",
    "')",
    "        for i, line in enumerate(lines):",
    "            if line.strip():",
    "                name, total, used = line.split(', ')",
    "                print(f'GPU {i}: {name.strip()} ({used}MB / {total}MB used)')",
    "    else:",
    "        print('",
    "nvidia-smi not available or failed')",
    "except (subprocess.TimeoutExpired, FileNotFoundError):",
    "    print('",
    "nvidia-smi command not found or timed out')",
    "except Exception as e:",
    "    print(f'",
    "nvidia-smi error: {e}')",
    "",
    "print('",
    "=== Summary ===')",
    "if torch.cuda.is_available():",
    "    print('\u2705 GPU is available and ready for use!')",
    "else:",
    "    print('\u26a0\ufe0f  No GPU available, will use CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install System Dependencies\n",
    "\n",
    "First, we need to install the required system packages and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ffmpeg for audio/video processing\n",
    "!apt-get update && apt-get install -y ffmpeg\n",
    "\n",
    "# Install git and other utilities\n",
    "!apt-get install -y git curl wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone the WhisperX FastAPI Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/pavelzbornik/whisperX-FastAPI.git\n",
    "!cd whisperX-FastAPI && ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Python Virtual Environment and Install Dependencies\n",
    "\n",
    "We'll install PyTorch with CUDA support and all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and activate a virtual environment\n",
    "!cd whisperX-FastAPI && python -m venv venv\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install --upgrade pip\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install project requirements\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install -r requirements/dev.txt\n",
    "\n",
    "# Install additional packages for Colab environment\n",
    "!cd whisperX-FastAPI && source venv/bin/activate && pip install colorlog pyngrok python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Environment Variables\n",
    "\n",
    "Configure the required environment variables for WhisperX. You'll need to enter your Hugging Face API token to access the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enter your Hugging Face token here\n",
    "HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Choose Whisper model size\n",
    "WHISPER_MODEL = input(\"Enter Whisper model size (default: tiny): \") or \"tiny\"\n",
    "\n",
    "# Set log level\n",
    "LOG_LEVEL = \"INFO\"\n",
    "\n",
    "# Create .env file\n",
    "env_content = f\"\"\"HF_TOKEN={HF_TOKEN}\n",
    "WHISPER_MODEL={WHISPER_MODEL}\n",
    "LOG_LEVEL={LOG_LEVEL}\n",
    "DEVICE=cuda\n",
    "COMPUTE_TYPE=float16\n",
    "DB_URL=sqlite:///records.db\n",
    "\"\"\"\n",
    "\n",
    "with open(\"whisperX-FastAPI/.env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"Environment configuration completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install and Configure Cloudflare Tunnel\n",
    "\n",
    "We'll use Cloudflare tunnels to expose the FastAPI service to the internet, allowing you to access it from your browser or other clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install cloudflared\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb\n",
    "\n",
    "print(\"Cloudflare tunnel client installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start the FastAPI Service\n",
    "\n",
    "Now we'll run the FastAPI application in the background and expose it through the Cloudflare tunnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run FastAPI in the background",
    "import subprocess",
    "import time",
    "import threading",
    "import os",
    "import requests",
    "import IPython.display as display",
    "from IPython.display import clear_output",
    "",
    "# Start FastAPI server in a separate process",
    "def start_fastapi_server():",
    "    print(\"Starting FastAPI server...\")",
    "    try:",
    "        # Change to the project directory",
    "        os.chdir(\"whisperX-FastAPI\")",
    "        ",
    "        # Command to start the FastAPI application with virtual environment",
    "        command = \"bash -c 'source venv/bin/activate && uvicorn app.main:app --host 0.0.0.0 --port 8000 --log-config app/uvicorn_log_conf.yaml'\"",
    "        ",
    "        # Start the process",
    "        process = subprocess.Popen(",
    "            command, ",
    "            shell=True, ",
    "            stdout=subprocess.PIPE, ",
    "            stderr=subprocess.PIPE,",
    "            universal_newlines=True,",
    "            preexec_fn=os.setsid  # Create new process group",
    "        )",
    "        ",
    "        print(\"FastAPI server process started. Process ID:\", process.pid)",
    "        time.sleep(3)  # Give some time for the server to start",
    "        ",
    "        # Change back to original directory",
    "        os.chdir(\"..\")",
    "        ",
    "        return process",
    "        ",
    "    except Exception as e:",
    "        print(f\"Error in start_fastapi_server: {e}\")",
    "        # Make sure we change back to original directory",
    "        try:",
    "            os.chdir(\"..\")",
    "        except:",
    "            pass",
    "        return None",
    "",
    "# Wait for FastAPI HTTP API to be ready",
    "def wait_for_fastapi(timeout=60):",
    "    print(\"Waiting for FastAPI to become ready...\")",
    "    for i in range(timeout):",
    "        try:",
    "            # Try both root endpoint and docs endpoint",
    "            response = requests.get(\"http://localhost:8000/\", timeout=2)",
    "            if response.status_code in [200, 404, 422]:  # 422 is expected for root endpoint",
    "                print(f\"\u2705 FastAPI is ready! (after {i+1}s)\")",
    "                return True",
    "        except requests.exceptions.RequestException:",
    "            pass",
    "        ",
    "        print(f\"\u23f3 Waiting for FastAPI to start... {i+1}s\")",
    "        time.sleep(1)",
    "    ",
    "    print(\"\u274c FastAPI did not start within timeout period\")",
    "    return False",
    "",
    "# Start the services",
    "fastapi_process = None",
    "",
    "try:",
    "    print(\"=== Starting FastAPI Service ===\")",
    "    fastapi_process = start_fastapi_server()",
    "    ",
    "    if fastapi_process:",
    "        # Wait for the service to be ready",
    "        if wait_for_fastapi():",
    "            print(\"\\n\u2705 FastAPI service is running successfully!\")",
    "            print(\"\ud83d\udcdd API Documentation: http://localhost:8000/docs\")",
    "            print(\"\ud83d\udd17 API Root: http://localhost:8000/\")",
    "            ",
    "            # Display clickable links in notebook",
    "            display.display(display.HTML('''",
    "                <div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 5px; background-color: #f9fff9;\">",
    "                    <h3>\ud83c\udf89 FastAPI is Ready!</h3>",
    "                    <p><strong>API Documentation:</strong> <a href=\"http://localhost:8000/docs\" target=\"_blank\">http://localhost:8000/docs</a></p>",
    "                    <p><strong>API Root:</strong> <a href=\"http://localhost:8000/\" target=\"_blank\">http://localhost:8000/</a></p>",
    "                    <p><em>Note: These links work if you're running locally. For Colab, you'll need to use the Cloudflare tunnel.</em></p>",
    "                </div>",
    "            '''))",
    "        else:",
    "            print(\"\u274c FastAPI failed to start properly\")",
    "            if fastapi_process:",
    "                fastapi_process.terminate()",
    "    else:",
    "        print(\"\u274c Failed to start FastAPI process\")",
    "        ",
    "except Exception as e:",
    "    print(f\"\u274c Error starting FastAPI service: {str(e)}\")",
    "    if fastapi_process:",
    "        try:",
    "            fastapi_process.terminate()",
    "        except:",
    "            pass",
    "",
    "# Store the process globally so it can be accessed later",
    "globals()[\"fastapi_process\"] = fastapi_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor GPU Usage\n",
    "\n",
    "You can monitor GPU usage while the service is running to ensure it's properly utilizing the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU usage\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the API\n",
    "\n",
    "Here's an example of how to use the API to transcribe an audio file uploaded to Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.colab import files\n",
    "import os",
    "\n",
    "# Function to upload a file to the WhisperX API\n",
    "def transcribe_audio(file_path, api_url):\n",
    "    # API endpoint for speech-to-text\n",
    "    endpoint = f\"{api_url}/speech-to-text\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        \"model\": WHISPER_MODEL,  # Use the same model as configured earlier\n",
    "        \"language\": \"en\"  # Change this to match your audio language\n",
    "    }\n",
    "    \n",
    "    # Create the multipart form data\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        files = {\"audio_file\": (os.path.basename(file_path), audio_file)}\n",
    "        response = requests.post(endpoint, params=params, files=files)\n",
    "    \n",
    "    # Return the response\n",
    "    return response.json()\n",
    "\n",
    "# Upload an audio file\n",
    "print(\"Please upload an audio file (supported formats: mp3, wav, m4a, etc.)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Process the uploaded file\n",
    "if uploaded:\n",
    "    file_name = list(uploaded.keys())[0]\n",
    "    \n",
    "    # Check if tunnel URL is available\n",
    "    if tunnel_url:\n",
    "        print(f\"Transcribing {file_name}...\")\n",
    "        result = transcribe_audio(file_name, tunnel_url)\n",
    "        \n",
    "        # Display the identifier for checking the task status\n",
    "        if \"identifier\" in result:\n",
    "            print(f\"\\nTask identifier: {result['identifier']}\")\n",
    "            print(f\"\\nCheck the task status at: {tunnel_url}/task/{result['identifier']}\")\n",
    "            display.display(display.HTML(f'<a href=\"{tunnel_url}/task/{result[\"identifier\"]}\" target=\"_blank\">Check Task Status</a>'))\n",
    "        else:\n",
    "            print(\"Error:\", result)\n",
    "    else:\n",
    "        print(\"Tunnel URL is not available. Make sure the FastAPI service and Cloudflare tunnel are running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check Task Status\n",
    "\n",
    "Use this cell to check the status of your transcription task using the task identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check task status\n",
    "def check_task_status(task_id, api_url):\n",
    "    endpoint = f\"{api_url}/task/{task_id}\"\n",
    "    response = requests.get(endpoint)\n",
    "    return response.json()\n",
    "\n",
    "# Enter task identifier\n",
    "task_id = input(\"Enter task identifier: \")\n",
    "\n",
    "# Check task status\n",
    "if tunnel_url and task_id:\n",
    "    status = check_task_status(task_id, tunnel_url)\n",
    "    print(\"Task Status:\")\n",
    "    import json\n",
    "    print(json.dumps(status, indent=2))\n",
    "else:\n",
    "    print(\"Tunnel URL or task ID is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Shutdown Services\n",
    "\n",
    "When you're done, use this cell to properly shut down the services and free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shut down services\n",
    "def shutdown_services():\n",
    "    global fastapi_process, tunnel_process\n",
    "    \n",
    "    print(\"Shutting down services...\")\n",
    "    \n",
    "    # Terminate Cloudflare tunnel\n",
    "    if tunnel_process:\n",
    "        tunnel_process.terminate()\n",
    "        tunnel_process.wait()\n",
    "        print(\"Cloudflare tunnel stopped.\")\n",
    "    \n",
    "    # Terminate FastAPI server\n",
    "    if fastapi_process:\n",
    "        fastapi_process.terminate()\n",
    "        fastapi_process.wait()\n",
    "        print(\"FastAPI server stopped.\")\n",
    "    \n",
    "    print(\"All services have been shut down.\")\n",
    "\n",
    "# Shut down the services\n",
    "shutdown_services()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup\n",
    "\n",
    "Finally, clean up any temporary files and free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "!rm -f cloudflared-linux-amd64.deb\n",
    "\n",
    "# Free up GPU memory (if any is still in use)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared.\")\n",
    "\n",
    "print(\"Cleanup completed. You can now close this notebook or run it again if needed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}