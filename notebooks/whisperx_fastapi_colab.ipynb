{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaansDole/whisperX-FastAPI/blob/main/notebooks/whisperx_fastapi_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgRmhCYcFPv9"
      },
      "source": [
        "# WhisperX FastAPI on Google Colab\n",
        "\n",
        "This notebook sets up and runs the WhisperX FastAPI project on Google Colab, utilizing its GPU for speech-to-text processing.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Speech-to-text transcription\n",
        "- Audio alignment\n",
        "- Speaker diarization\n",
        "- Combined services\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Google Colab with GPU runtime\n",
        "- Hugging Face token for model access\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Make sure you're running this notebook with GPU runtime\n",
        "2. Execute each cell in order\n",
        "\n",
        "Finally, run the silent audio below to keep the session alive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "UowbTabZFS0k",
        "outputId": "5d97f692-0396-4060-86e1-574c8362cba3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0KVaZ_zFPwC"
      },
      "source": [
        "## 1. Install System Dependencies\n",
        "\n",
        "First, we need to install the required system packages and utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9foV_tYFPwC"
      },
      "outputs": [],
      "source": [
        "# Install ffmpeg for audio/video processing\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "\n",
        "# Install git and other utilities\n",
        "!apt-get install -y git curl wget\n",
        "\n",
        "!apt update\n",
        "!apt install libcudnn8 libcudnn8-dev -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfCgeb4JFPwC"
      },
      "source": [
        "## 2. Clone the WhisperX FastAPI Repository\n",
        "\n",
        "You can ignore the errors below, they are just false alarm.\n",
        "\n",
        "```\n",
        "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
        "chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlMBW9VAFPwC",
        "outputId": "f5451c56-bbbb-47dc-b255-cabf61f9abd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Cloning into 'whisperX-FastAPI'...\n",
            "remote: Enumerating objects: 1382, done.\u001b[K\n",
            "remote: Counting objects: 100% (441/441), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 1382 (delta 315), reused 269 (delta 261), pack-reused 941 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1382/1382), 40.59 MiB | 10.79 MiB/s, done.\n",
            "Resolving deltas: 100% (764/764), done.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "total 20\n",
            "drwxr-xr-x  1 root root 4096 Jun 26 16:13 .\n",
            "drwxr-xr-x  1 root root 4096 Jun 26 15:50 ..\n",
            "drwxr-xr-x  4 root root 4096 Jun 24 13:38 .config\n",
            "drwxr-xr-x  1 root root 4096 Jun 24 13:38 sample_data\n",
            "drwxr-xr-x 11 root root 4096 Jun 26 16:13 whisperX-FastAPI\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!rm -rf /content/whisperX-FastAPI\n",
        "!cd /content && git clone https://github.com/LaansDole/whisperX-FastAPI.git\n",
        "!ls -la /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueqDc01EFPwD"
      },
      "source": [
        "## 3. Install Dependencies\n",
        "\n",
        "We'll install PyTorch with CUDA support and all required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test script to verify PyTorch installation and CUDA availability\n",
        "\"\"\"\n",
        "import sys\n",
        "\n",
        "def test_torch_installation():\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"✓ PyTorch installed successfully: {torch.__version__}\")\n",
        "\n",
        "        # Test CUDA availability\n",
        "        if hasattr(torch, 'cuda'):\n",
        "            if torch.cuda.is_available():\n",
        "                print(f\"✓ CUDA is available: {torch.cuda.get_device_name(0)}\")\n",
        "                print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
        "            else:\n",
        "                print(\"⚠ CUDA is not available, will use CPU\")\n",
        "        else:\n",
        "            print(\"✗ torch.cuda module not found - PyTorch installation is corrupted\")\n",
        "            return False\n",
        "\n",
        "        # Test basic tensor operations\n",
        "        x = torch.randn(3, 3)\n",
        "        print(f\"✓ Basic tensor operations work: {x.shape}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"✗ Failed to import PyTorch: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"✗ PyTorch test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def test_numpy_installation():\n",
        "    try:\n",
        "        import numpy as np\n",
        "        print(f\"✓ NumPy installed successfully: {np.__version__}\")\n",
        "\n",
        "        # Test basic operations\n",
        "        arr = np.array([1, 2, 3])\n",
        "        print(f\"✓ Basic NumPy operations work: {arr.shape}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"✗ Failed to import NumPy: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"✗ NumPy test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing PyTorch and NumPy installation...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    numpy_ok = test_numpy_installation()\n",
        "    torch_ok = test_torch_installation()\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    if numpy_ok and torch_ok:\n",
        "        print(\"✓ All tests passed! Environment is ready.\")\n",
        "    else:\n",
        "        print(\"✗ Some tests failed. Please check the installation.\")"
      ],
      "metadata": {
        "id": "8UHM5_KmUEEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJrF6J1tFPwD"
      },
      "outputs": [],
      "source": [
        "# Install project requirements\n",
        "!cd /content/whisperX-FastAPI && pip install -r requirements/prod.txt\n",
        "\n",
        "# Install additional packages for Colab environment\n",
        "!cd /content/whisperX-FastAPI && pip install colorlog pyngrok python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJQd3yTqFPwD"
      },
      "source": [
        "## 4. Set Up Environment Variables\n",
        "\n",
        "Configure the required environment variables for WhisperX. You'll need to enter your Hugging Face API token to access the models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Hugging Face API token\n",
        "1. Go to your Hugging Face token settings page.\n",
        "2. Select the token you are using.\n",
        "3. Under the \"Token permissions\" section, make sure that \"Read access to public gated repositories\" is enabled **[IMPORTANT]**.\n",
        "4. Save the changes to your token.\n",
        "\n"
      ],
      "metadata": {
        "id": "RCwqHIbMrmC8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24342fd9"
      },
      "source": [
        "To add your Hugging Face token as a secret in Google Colab:\n",
        "\n",
        "1.  Go to your Hugging Face settings page: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "2.  Create a new token or copy an existing one.\n",
        "3.  In your Google Colab notebook, click on the \"🔑 Secrets\" tab in the left sidebar.\n",
        "4.  Click on \"Add new secret\".\n",
        "5.  For the \"Name\" field, enter `HF_TOKEN`.\n",
        "6.  For the \"Value\" field, paste your Hugging Face token.\n",
        "7.  Make sure the \"Notebook access\" toggle is enabled for this notebook.\n",
        "8.  Restart your Colab session by going to \"Runtime\" -> \"Restart session\".\n",
        "\n",
        "Once you have followed these steps, the `HF_TOKEN` secret will be available in your notebook and the warning message should disappear after restarting the runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MokzptrhTPKJ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure that you clear the output cell below whenever you save this notebook to GitHub"
      ],
      "metadata": {
        "id": "JUZ5TBMQuX5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "def download_model(model_name, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Downloads a model from the Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model to download.\n",
        "        cache_dir (str, optional): The directory to cache the model in. Defaults to None.\n",
        "    \"\"\"\n",
        "    print(f\"Downloading model: {model_name}\")\n",
        "    try:\n",
        "        snapshot_download(\n",
        "            repo_id=model_name,\n",
        "            cache_dir=cache_dir,\n",
        "            token=os.environ.get(\"HF_TOKEN\"),\n",
        "        )\n",
        "        print(f\"Model '{model_name}' downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model '{model_name}': {e}\")\n",
        "\n",
        "# Directly call the download function with the desired model name\n",
        "download_model(model_name=\"pyannote/speaker-diarization-3.1\", cache_dir=\"models/pyannote/speaker-diarization-3.1\")"
      ],
      "metadata": {
        "id": "wos4bjuapKFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CyTAXbREFPwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bea8ed2-139a-4abf-c8f9-da7a1be11276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed directory to /content/whisperX-FastAPI\n",
            "Enter Whisper model size (default: tiny): small\n",
            ".env file created successfully.\n",
            "Environment configuration process completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Change to the whisperX-FastAPI directory\n",
        "try:\n",
        "    os.chdir(\"/content/whisperX-FastAPI\")\n",
        "    print(\"Changed directory to /content/whisperX-FastAPI\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: /content/whisperX-FastAPI directory not found. Please ensure the repository was cloned successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while changing directory: {e}\")\n",
        "\n",
        "\n",
        "# Get Hugging Face token from Colab secrets\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    if not HF_TOKEN:\n",
        "        print(\"Warning: HF_TOKEN secret not found. Please add it to Colab secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving HF_TOKEN from secrets: {e}\")\n",
        "    HF_TOKEN = \"\" # Set empty to avoid errors later\n",
        "\n",
        "# Choose Whisper model size\n",
        "WHISPER_MODEL = input(\"Enter Whisper model size (default: tiny): \") or \"tiny\"\n",
        "\n",
        "# Set log level\n",
        "LOG_LEVEL = \"INFO\"\n",
        "\n",
        "# Create .env file\n",
        "env_content = f\"\"\"HF_TOKEN={HF_TOKEN}\n",
        "WHISPER_MODEL={WHISPER_MODEL}\n",
        "LOG_LEVEL={LOG_LEVEL}\n",
        "DEVICE=cuda\n",
        "COMPUTE_TYPE=float16\n",
        "DB_URL=sqlite:///records.db\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    with open(\".env\", \"w\") as f:\n",
        "        f.write(env_content)\n",
        "    print(\".env file created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error writing .env file: {e}\")\n",
        "\n",
        "print(\"Environment configuration process completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env"
      ],
      "metadata": {
        "id": "SFbTdZARZ-kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji-K4diXFPwE"
      },
      "source": [
        "## 5. Start the FastAPI Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-VVKLbgTPKK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from google.colab.output import serve_kernel_port_as_iframe\n",
        "\n",
        "# --- Configuration ---\n",
        "PORT = 8000\n",
        "LOG_CONFIG_PATH = \"app/uvicorn_log_conf.yaml\"\n",
        "APP_MODULE = \"app.main:app\"\n",
        "\n",
        "# --- Global variable to hold the server process ---\n",
        "server_process = None\n",
        "\n",
        "def kill_port(port):\n",
        "    \"\"\"Kills any process listening on the given port.\"\"\"\n",
        "    print(f\"Checking for and terminating any process on port {port}...\")\n",
        "    try:\n",
        "        result = subprocess.run([\"lsof\", \"-ti\", f\":{port}\"], capture_output=True, text=True)\n",
        "        if result.stdout:\n",
        "            pids = result.stdout.strip().split('\\n')\n",
        "            for pid in pids:\n",
        "                try:\n",
        "                    os.kill(int(pid), signal.SIGKILL)\n",
        "                    print(f\"Killed process {pid} on port {port}.\")\n",
        "                except (ProcessLookupError, ValueError):\n",
        "                    pass  # Process already gone\n",
        "    except FileNotFoundError:\n",
        "        print(\"`lsof` command not found. Skipping port clearing.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while trying to kill port {port}: {e}\")\n",
        "\n",
        "def start_server():\n",
        "    \"\"\"Starts the Uvicorn server in a background thread.\"\"\"\n",
        "    global server_process\n",
        "\n",
        "    # First, ensure the port is free\n",
        "    kill_port(PORT)\n",
        "\n",
        "    # Command to start Uvicorn\n",
        "    command = [\n",
        "        \"uvicorn\",\n",
        "        APP_MODULE,\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", str(PORT),\n",
        "        \"--log-config\", LOG_CONFIG_PATH,\n",
        "        \"--log-level\", \"info\"\n",
        "    ]\n",
        "\n",
        "    # Start the server as a background process\n",
        "    print(\"Starting FastAPI server...\")\n",
        "    server_process = subprocess.Popen(command)\n",
        "    print(f\"Server process started with PID: {server_process.pid}\")\n",
        "\n",
        "    # Wait a moment for the server to initialize\n",
        "    time.sleep(12)\n",
        "\n",
        "    # Expose the port to a public URL\n",
        "    print(f\"Exposing port {PORT} as an iframe...\")\n",
        "    serve_kernel_port_as_iframe(port=PORT, height=800)\n",
        "\n",
        "def stop_server():\n",
        "    \"\"\"Stops the background Uvicorn server.\"\"\"\n",
        "    global server_process\n",
        "    if server_process:\n",
        "        print(f\"Stopping server process with PID: {server_process.pid}...\")\n",
        "        server_process.terminate()\n",
        "        try:\n",
        "            # Wait for the process to terminate\n",
        "            server_process.wait(timeout=10)\n",
        "            print(\"Server stopped successfully.\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"Server did not terminate gracefully. Forcing shutdown...\")\n",
        "            server_process.kill()\n",
        "            print(\"Server forced to shut down.\")\n",
        "        server_process = None\n",
        "    else:\n",
        "        print(\"Server is not running.\")\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        start_server()\n",
        "        # The server is running in the background.\n",
        "        # The script will keep running, allowing the server to stay active.\n",
        "        # To stop the server, you would call stop_server() in another cell.\n",
        "        print(\"\\nServer is running in the background.\")\n",
        "        print(\"To stop the server, call the stop_server() function.\")\n",
        "        # Keep the main thread alive\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nKeyboard interrupt received. Shutting down server...\")\n",
        "        stop_server()\n",
        "        print(\"Shutdown complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d-lXHtOKpJ17"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}