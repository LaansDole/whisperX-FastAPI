{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LaansDole/whisperX-FastAPI/blob/main/notebooks/whisperx_fastapi_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgRmhCYcFPv9"
   },
   "source": [
    "# WhisperX FastAPI on Google Colab\n",
    "\n",
    "This notebook sets up and runs the WhisperX FastAPI project on Google Colab, utilizing its GPU for speech-to-text processing. The API service is exposed through a Cloudflare tunnel to allow external access.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Speech-to-text transcription\n",
    "- Audio alignment\n",
    "- Speaker diarization\n",
    "- Combined services\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab with GPU runtime\n",
    "- Hugging Face token for model access\n",
    "- Cloudflare account (free tier works fine)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Make sure you're running this notebook with GPU runtime\n",
    "2. Execute each cell in order\n",
    "3. Use the Cloudflare tunnel URL to access the API\n",
    "\n",
    "Let's start by checking if we have GPU access and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "UowbTabZFS0k",
    "outputId": "5d97f692-0396-4060-86e1-574c8362cba3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0KVaZ_zFPwC"
   },
   "source": [
    "## 1. Install System Dependencies\n",
    "\n",
    "First, we need to install the required system packages and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9foV_tYFPwC"
   },
   "outputs": [],
   "source": [
    "# Install ffmpeg for audio/video processing\n",
    "!apt-get update && apt-get install -y ffmpeg\n",
    "\n",
    "# Install git and other utilities\n",
    "!apt-get install -y git curl wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfCgeb4JFPwC"
   },
   "source": [
    "## 2. Clone the WhisperX FastAPI Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlMBW9VAFPwC",
    "outputId": "4b2bff6e-35df-41a7-fb49-66cfba9b8002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'whisperX-FastAPI'...\n",
      "remote: Enumerating objects: 1265, done.\u001b[K\n",
      "remote: Counting objects: 100% (454/454), done.\u001b[K\n",
      "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
      "remote: Total 1265 (delta 316), reused 258 (delta 258), pack-reused 811 (from 2)\u001b[K\n",
      "Receiving objects: 100% (1265/1265), 40.47 MiB | 40.79 MiB/s, done.\n",
      "Resolving deltas: 100% (682/682), done.\n",
      "total 96\n",
      "drwxr-xr-x 9 root root 4096 Jun 24 17:28 .\n",
      "drwxr-xr-x 1 root root 4096 Jun 24 17:28 ..\n",
      "drwxr-xr-x 4 root root 4096 Jun 24 17:28 app\n",
      "drwxr-xr-x 2 root root 4096 Jun 24 17:28 .devcontainer\n",
      "-rw-r--r-- 1 root root  531 Jun 24 17:28 docker-compose.yml\n",
      "-rw-r--r-- 1 root root 1627 Jun 24 17:28 dockerfile\n",
      "-rw-r--r-- 1 root root  331 Jun 24 17:28 .dockerignore\n",
      "drwxr-xr-x 8 root root 4096 Jun 24 17:28 .git\n",
      "drwxr-xr-x 3 root root 4096 Jun 24 17:28 .github\n",
      "-rw-r--r-- 1 root root   52 Jun 24 17:28 .gitignore\n",
      "-rw-r--r-- 1 root root  207 Jun 24 17:28 .gitleaks.toml\n",
      "-rw-r--r-- 1 root root 1070 Jun 24 17:28 LICENSE\n",
      "-rw-r--r-- 1 root root   39 Jun 24 17:28 .markdownlint.json\n",
      "-rw-r--r-- 1 root root 1963 Jun 24 17:28 .pre-commit-config.yaml\n",
      "-rw-r--r-- 1 root root  532 Jun 24 17:28 pyproject.toml\n",
      "-rw-r--r-- 1 root root 9405 Jun 24 17:28 README.md\n",
      "drwxr-xr-x 2 root root 4096 Jun 24 17:28 requirements\n",
      "-rw-r--r-- 1 root root  153 Jun 24 17:28 requirements.txt\n",
      "-rw-r--r-- 1 root root   86 Jun 24 17:28 setup.cfg\n",
      "-rwxr-xr-x 1 root root   77 Jun 24 17:28 start.sh\n",
      "drwxr-xr-x 3 root root 4096 Jun 24 17:28 tests\n",
      "drwxr-xr-x 2 root root 4096 Jun 24 17:28 .vscode\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!rm -rf whisperX-FastAPI\n",
    "!git clone https://github.com/pavelzbornik/whisperX-FastAPI.git\n",
    "!cd whisperX-FastAPI && ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueqDc01EFPwD"
   },
   "source": [
    "## 3. Install Dependencies\n",
    "\n",
    "We'll install PyTorch with CUDA support and all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJrF6J1tFPwD"
   },
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!cd whisperX-FastAPI && pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install project requirements\n",
    "!cd whisperX-FastAPI && pip install -r requirements/prod.txt\n",
    "\n",
    "# Install additional packages for Colab environment\n",
    "!cd whisperX-FastAPI && pip install colorlog pyngrok python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJQd3yTqFPwD"
   },
   "source": [
    "## 4. Set Up Environment Variables\n",
    "\n",
    "Configure the required environment variables for WhisperX. You'll need to enter your Hugging Face API token to access the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyTAXbREFPwD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if we're already in the whisperX-FastAPI directory\n",
    "current_dir = os.path.basename(os.getcwd())\n",
    "if current_dir != \"whisperX-FastAPI\":\n",
    "    os.chdir(\"whisperX-FastAPI\")\n",
    "    print(f\"Changed directory to whisperX-FastAPI\")\n",
    "else:\n",
    "    print(\"Already in whisperX-FastAPI directory\")\n",
    "\n",
    "# Enter your Hugging Face token here\n",
    "HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Choose Whisper model size\n",
    "WHISPER_MODEL = input(\"Enter Whisper model size (default: tiny): \") or \"tiny\"\n",
    "\n",
    "# Set log level\n",
    "LOG_LEVEL = \"INFO\"\n",
    "\n",
    "# Create .env file\n",
    "env_content = f\"\"\"HF_TOKEN={HF_TOKEN}\n",
    "WHISPER_MODEL={WHISPER_MODEL}\n",
    "LOG_LEVEL={LOG_LEVEL}\n",
    "DEVICE=cuda\n",
    "COMPUTE_TYPE=float16\n",
    "DB_URL=sqlite:///records.db\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"Environment configuration completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download; snapshot_download(repo_id='openai/whisper-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji-K4diXFPwE"
   },
   "source": [
    "## 5. Start the FastAPI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "from google.colab.output import serve_kernel_port_as_iframe\n",
    "\n",
    "# --- Configuration ---\n",
    "PORT = 8000\n",
    "LOG_CONFIG_PATH = \"app/uvicorn_log_conf.yaml\"\n",
    "APP_MODULE = \"app.main:app\"\n",
    "\n",
    "# --- Global variable to hold the server process ---\n",
    "server_process = None\n",
    "\n",
    "def kill_port(port):\n",
    "    \"\"\"Kills any process listening on the given port.\"\"\"\n",
    "    print(f\"Checking for and terminating any process on port {port}...\")\n",
    "    try:\n",
    "        result = subprocess.run([\"lsof\", \"-ti\", f\":{port}\"], capture_output=True, text=True)\n",
    "        if result.stdout:\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            for pid in pids:\n",
    "                try:\n",
    "                    os.kill(int(pid), signal.SIGKILL)\n",
    "                    print(f\"Killed process {pid} on port {port}.\")\n",
    "                except (ProcessLookupError, ValueError):\n",
    "                    pass  # Process already gone\n",
    "    except FileNotFoundError:\n",
    "        print(\"`lsof` command not found. Skipping port clearing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while trying to kill port {port}: {e}\")\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Starts the Uvicorn server in a background thread.\"\"\"\n",
    "    global server_process\n",
    "\n",
    "    # Ensure we are in the correct directory\n",
    "    if os.path.basename(os.getcwd()) != \"whisperX-FastAPI\":\n",
    "        os.chdir(\"whisperX-FastAPI\")\n",
    "        print(\"Changed directory to whisperX-FastAPI\")\n",
    "\n",
    "    # First, ensure the port is free\n",
    "    kill_port(PORT)\n",
    "\n",
    "    # Command to start Uvicorn\n",
    "    command = [\n",
    "        \"uvicorn\",\n",
    "        APP_MODULE,\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", str(PORT),\n",
    "        \"--log-config\", LOG_CONFIG_PATH,\n",
    "        \"--log-level\", \"info\"\n",
    "    ]\n",
    "\n",
    "    # Start the server as a background process\n",
    "    print(\"Starting FastAPI server...\")\n",
    "    server_process = subprocess.Popen(command)\n",
    "    print(f\"Server process started with PID: {server_process.pid}\")\n",
    "\n",
    "    # Wait a moment for the server to initialize\n",
    "    time.sleep(12)\n",
    "\n",
    "    # Expose the port to a public URL\n",
    "    print(f\"Exposing port {PORT} as an iframe...\")\n",
    "    serve_kernel_port_as_iframe(port=PORT, height=800)\n",
    "\n",
    "def stop_server():\n",
    "    \"\"\"Stops the background Uvicorn server.\"\"\"\n",
    "    global server_process\n",
    "    if server_process:\n",
    "        print(f\"Stopping server process with PID: {server_process.pid}...\")\n",
    "        server_process.terminate()\n",
    "        try:\n",
    "            # Wait for the process to terminate\n",
    "            server_process.wait(timeout=10)\n",
    "            print(\"Server stopped successfully.\")\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Server did not terminate gracefully. Forcing shutdown...\")\n",
    "            server_process.kill()\n",
    "            print(\"Server forced to shut down.\")\n",
    "        server_process = None\n",
    "    else:\n",
    "        print(\"Server is not running.\")\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        start_server()\n",
    "        # The server is running in the background.\n",
    "        # The script will keep running, allowing the server to stay active.\n",
    "        # To stop the server, you would call stop_server() in another cell.\n",
    "        print(\"\\nServer is running in the background.\")\n",
    "        print(\"To stop the server, call the stop_server() function.\")\n",
    "        # Keep the main thread alive\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt received. Shutting down server...\")\n",
    "        stop_server()\n",
    "        print(\"Shutdown complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
